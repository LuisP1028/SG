<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DDIA Study Guide // TERMINAL MODE</title>
    <style>
        :root {
            --bg-color: #050505;
            --text-color: #00ff33;
            --dim-color: #004411;
            --border-width: 2px;
            --shadow-offset: 4px;
            --font-main: "Courier New", Courier, monospace;
            --font-headings: "Arial Black", Gadget, sans-serif;
        }

        /* --- GLOBAL STYLES --- */
        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            padding: 20px;
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: var(--font-main);
            line-height: 1.4;
            /* Dither Background Pattern (Dark on Dark) */
            background-image: 
                linear-gradient(45deg, #111111 25%, transparent 25%), 
                linear-gradient(-45deg, #111111 25%, transparent 25%), 
                linear-gradient(45deg, transparent 75%, #111111 75%), 
                linear-gradient(-45deg, transparent 75%, #111111 75%);
            background-size: 4px 4px;
            background-position: 0 0, 0 2px, 2px -2px, -2px 0px;
            min-height: 100vh;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            position: relative;
            background: var(--bg-color);
            border: var(--border-width) solid var(--text-color);
            box-shadow: var(--shadow-offset) var(--shadow-offset) 0px var(--dim-color);
            padding: 20px;
            z-index: 10;
        }

        /* --- TYPOGRAPHY --- */
        h1 {
            font-family: var(--font-headings);
            text-transform: uppercase;
            letter-spacing: -1px;
            font-size: 2.5rem;
            margin-top: 0;
            margin-bottom: 20px;
            border-bottom: 4px solid var(--text-color);
            padding-bottom: 10px;
            text-align: center;
            text-shadow: 2px 2px 0px var(--dim-color);
        }

        /* --- ACCORDION STYLES --- */
        details {
            margin-bottom: 15px;
            border: var(--border-width) solid var(--text-color);
            background: var(--bg-color);
            transition: all 0.1s;
        }

        summary {
            cursor: pointer;
            padding: 12px 15px;
            list-style: none; /* Hide default triangle */
            position: relative;
            font-weight: bold;
            display: flex;
            justify-content: space-between;
            align-items: center;
            user-select: none;
        }

        summary::-webkit-details-marker {
            display: none;
        }

        /* Custom Indicator */
        summary::after {
            content: "[ + ]";
            font-family: var(--font-main);
            font-weight: bold;
            color: var(--text-color);
        }

        details[open] > summary::after {
            content: "[ - ]";
        }

        /* --- HIERARCHY LEVELS --- */
        
        /* Level 1: Parts */
        details.part {
            box-shadow: 3px 3px 0px var(--dim-color);
        }
        
        details.part > summary {
            background-color: var(--text-color);
            color: var(--bg-color);
            font-family: var(--font-headings);
            font-size: 1.2rem;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        details.part > summary::after {
            color: var(--bg-color);
        }
        
        .part-content {
            padding: 15px;
            border-top: 2px solid var(--text-color);
            background-image: radial-gradient(var(--dim-color) 1px, transparent 0);
            background-size: 4px 4px;
        }

        .part-focus {
            font-style: italic;
            margin-bottom: 15px;
            padding: 10px;
            background: #001100;
            border: 1px dashed var(--text-color);
            font-size: 0.9rem;
        }

        /* Level 2: Chapters */
        details.chapter {
            margin-left: 0;
            box-shadow: 2px 2px 0px #000;
            border-color: var(--dim-color);
        }

        details.chapter > summary {
            background-color: #111;
            color: var(--text-color);
            font-size: 1.1rem;
            border-bottom: 1px solid transparent;
        }

        details.chapter[open] > summary {
            border-bottom: 2px solid var(--text-color);
        }

        .chapter-content {
            padding: 15px;
            background: var(--bg-color);
        }

        /* Level 3: Subsections (Concepts) */
        details.subsection {
            border: 1px solid var(--dim-color);
            box-shadow: none;
            margin-bottom: 10px;
        }

        details.subsection > summary {
            background-color: var(--bg-color);
            font-size: 1rem;
            border-bottom: 1px dotted var(--dim-color);
            color: var(--text-color);
        }
        
        details.subsection > summary:hover {
            background-color: #002200;
        }

        details.subsection[open] > summary {
            border-bottom: 1px solid var(--text-color);
            background-color: var(--text-color);
            color: black;
        }
        
        details.subsection[open] > summary::after {
            color: black;
        }

        .subsection-content {
            padding: 15px;
            font-size: 0.95rem;
            color: #ccffcc;
        }

        /* Content Formatting */
        .label {
            font-weight: 800;
            text-transform: uppercase;
            font-size: 0.8rem;
            background: var(--text-color);
            color: black;
            padding: 1px 4px;
            margin-right: 5px;
        }

        .trade-offs {
            margin-top: 8px;
            padding: 8px;
            background: #0a1a0a;
            border-left: 3px solid var(--text-color);
            font-style: italic;
            color: #aaffaa;
        }

        .use-cases, .examples {
            margin-top: 8px;
            font-family: var(--font-main);
            color: #66cc66;
        }

        ul {
            padding-left: 20px;
            margin: 5px 0;
        }

        /* --- CRT OVERLAY EFFECT --- */
        .scanlines {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            background: linear-gradient(
                to bottom,
                rgba(0, 255, 50, 0),
                rgba(0, 255, 50, 0) 50%,
                rgba(0, 0, 0, 0.2) 50%,
                rgba(0, 0, 0, 0.2)
            );
            background-size: 100% 4px;
            pointer-events: none;
            z-index: 999;
        }

        .noise {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            pointer-events: none;
            z-index: 998;
            opacity: 0.05;
            background-image: url('data:image/svg+xml;utf8,%3Csvg viewBox="0 0 200 200" xmlns="http://www.w3.org/2000/svg"%3E%3Cfilter id="noiseFilter"%3E%3CfeTurbulence type="fractalNoise" baseFrequency="0.65" numOctaves="3" stitchTiles="stitch"/%3E%3C/filter%3E%3Crect width="100%25" height="100%25" filter="url(%23noiseFilter)"/%3E%3C/svg%3E');
        }

        /* Mobile Responsiveness */
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .container { padding: 10px; }
            summary { font-size: 0.9rem; }
            .part-content, .chapter-content { padding: 10px; }
        }
    </style>
</head>
<body>

    <div class="scanlines"></div>
    <div class="noise"></div>

    <div class="container">
        <h1>DDIA Study Guide</h1>
        
        <!-- PART I -->
        <details class="part">
            <summary>Part I: Foundations of Data Systems</summary>
            <div class="part-content">
                <div class="part-focus">
                    <strong>Focus:</strong> Reliability, Scalability, Maintainability, Data Models, Storage, and Encoding.
                </div>

                <!-- Chapter 1 -->
                <details class="chapter">
                    <summary>1. System Design Goals (Chapter 1)</summary>
                    <div class="chapter-content">
                        
                        <details class="subsection">
                            <summary>Reliability</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Reliability ensures the system functions as expected despite hardware failures, software bugs, or human errors, using techniques like fault tolerance, replication, and recovery mechanisms to prevent catastrophic failures and maintain service availability. This foundational quality prioritizes correctness over time, often tested through deliberate fault injection.<br>
                                <div class="trade-offs"><span class="label">Context:</span> Redundancy increases cost/complexity. Chaos Engineering (e.g., Netflix Chaos Monkey) tests this.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Scalability</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Scalability describes a system's capacity to handle growing load by measuring response time percentiles and throughput, employing strategies like load balancing and partitioning to sustain performance without degradation as user demands or data volumes increase exponentially.<br>
                                <div class="trade-offs"><span class="label">Context:</span> Horizontal scaling (shared-nothing) adds network complexity vs. Vertical scaling (hardware limits).</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Maintainability</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Maintainability focuses on making systems operable, simple, and evolvable, incorporating operability for smooth operations, simplicity to reduce accidental complexity, and evolvability for easy adaptations, ensuring long-term viability through modular design and documentation.<br>
                                <div class="trade-offs"><span class="label">Context:</span> Abstractions hide complexity but can leak; requires good observability and automation.</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 2 -->
                <details class="chapter">
                    <summary>2. Data Models & Query Languages (Chapter 2)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Relational Model (SQL)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> The relational model structures data in tables with rows and columns, enforcing schemas and supporting declarative SQL queries for joins, filters, and aggregations, providing strong consistency and ACID properties ideal for structured, normalized data with complex relationships.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Excellent for joins/integrity; suffers "impedance mismatch" with OOP code; schema changes are heavy. Ideal for structured, normalized data.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Financial ledgers, personnel records, strictly structured data.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Document Model (NoSQL)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Document models store hierarchical, self-contained data structures like JSON documents, allowing flexible schemas inferred at read time, optimizing for locality in denormalized data access but requiring application-level handling for relationships and consistency.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> High data locality (one seek reads all); poor support for many-to-many joins.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> User profiles, content management, heterogeneous patient summaries.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Graph Model (Property Graph)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Graph models represent data as vertices and edges with properties, enabling efficient traversal queries via languages like Cypher, suited for interconnected data where relationships are first-class, supporting deep queries without costly joins in relational systems.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Unbeatable for complex many-to-many logic; difficult to shard across multiple nodes.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Social networks, fraud detection networks, knowledge graphs.</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 3 -->
                <details class="chapter">
                    <summary>3. Storage Engines (Chapter 3)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Log-Structured (LSM-Trees) [IMMUTABLE] [WRITE THROUGHPUT]</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Log-structured storage engines append writes to an immutable log, using in-memory memtables flushed to sorted SSTables on disk, with background compaction merging files to optimize read performance while handling high write volumes efficiently in big data scenarios.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> High write throughput; slower reads (checks multiple files); compaction causes latency spikes.</div>
                                <div class="examples"><span class="label">Examples:</span> Cassandra, LevelDB, RocksDB (high ingestion telemetry).</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Page-Oriented (B-Trees) [MUTABLE] [EFFICIENT RANGE SCANS] </summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Page-oriented engines like B-trees organize data in fixed-size blocks with balanced tree indices for range queries and updates, performing in-place modifications with write-ahead logging for crash recovery, balancing read/write performance in transactional databases.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Fast, predictable reads; write amplification (changing 1 byte rewrites 4KB).</div>
                                <div class="examples"><span class="label">Examples:</span> PostgreSQL, MySQL, SQL Server (transactional systems).</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Column-Oriented Storage</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Column-oriented storage groups values by column rather than row, enabling high compression ratios through run-length encoding and bitmap indices, accelerating analytical queries on large datasets by reading only relevant columns during scans and aggregations.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> High compression/fast analytics; excels at aggregating specific fields (e.g., "Sum Sales"), but joining multiple tables is often slower.</div>
                                <div class="trade-offs"><span class="label">RUN LENGTH ENCODING:</span> Compresses sequential repeats. Instead of storing [Red, Red, Red, Blue, Blue], the database stores (Red, 3), (Blue, 2)</div>
                                <div class="trade-offs"><span class="label">BITMAP ENCODING:</span>VERY SIMILAR TO ONEHOTENCODING. Creates a separate binary array (a string of 0s and 1s) for every unique value in a column. Bitmap encoding for low-cardinality: Values [red, blue, red, red, green] → bitmaps: red: 1011, blue: 0100, green: 0001. Each bit shows if row has that value. </div>
                                <div class="examples"><span class="label">Examples:</span> Redshift, Snowflake, ClickHouse (Data Warehousing/OLAP).</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 4 -->
                <details class="chapter">
                    <summary>4. Data Encoding (Chapter 4)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Binary with Schema (Avro/Protobuf/Thrift)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Schema-based binary formats define the layout once upfront. Data payloads then pack only raw values sequentially—no field names, delimiters, or whitespace—minimizing size using the known schema, supporting forward/backward compatibility0 through schema evolution rules, ideal for efficient serialization in distributed systems with evolving data structures.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Efficient size/speed; supports schema evolution; requires schema registry; not human-readable.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Microservices (gRPC), archival of massive datasets.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Textual Formats (JSON/XML)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Textual formats like JSON and XML provide human-readable, self-describing data interchange with nested structures and optional schemas, facilitating debugging and interoperability across languages, though at the cost of larger payloads and parsing overhead. Dominant standards for data interchange (APIs, microservices, and web requests) because they are human-readable<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Flexible and accessible; verbose (high space usage) and slower parsing.</div>
                            </div>
                        </details>

                    </div>
                </details>

            </div>
        </details>

        <!-- PART II -->
        <details class="part">
            <summary>Part II: Distributed Data</summary>
            <div class="part-content">
                <div class="part-focus">
                    <strong>Focus:</strong> Replication, Partitioning, Transactions, Consistency, and Consensus.
                </div>

                <!-- Chapter 5 -->
                <details class="chapter">
                    <summary>5. Replication Models (Chapter 5)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Single-Leader</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> In single-leader replication, writes hit a single primary node first. This node organizes the operations and copies them to follower nodes. This keeps your data consistent, but it introduces complexity when you need to recover from a primary node failure.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Simple logic; leader is a write bottleneck and single point of failure.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Single-leader replication use cases boil down to when sequential processing is absolutely vital [e.g. serialization]</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Multi-Leader</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> In multi-leader setups, you can save data to several servers at the same time, often spread across the world. Because replication happens in the background, you have to handle conflicts using tools like CRDTs [Conflict Free Replicated Data Types] or custom code, but this gives you massive write availability for global systems<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> High availability/partition tolerance; complex conflict resolution (e.g., Last Write Wins).</div>
                                <div class="trade-offs"><span class="label">CDRTs:</span> Operation-based CRDTs: Replicas exchange operations (e.g., "insert char at position 5"). These operations are designed to be commutative when applied, enabling concurrent updates without coordination.
                                    State-based CRDTs (CvRDTs): Replicas exchange full state, merged via an inflationary, monotonic function (e.g., least upper bound in a join-semilattice). Merges are commutative, associative, and idempotent.</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Collaborative editing,  geo-distributed setups, low-latency local writes .</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Leaderless (AVAILABILITY > CONSISTENCY) [GREAT FOR SPOTTY ENVIRONMENTS]</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Leaderless replication distributes writes and reads across nodes using quorums (W + R > N) for consistency, employing hinted handoffs and anti-entropy for repair, prioritizing availability over strict ordering in highly fault-tolerant, partition-resilient environments.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Max availability; hard to ensure linearizability; risk of stale reads (requires read repair).</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Amazon Dynamo, Cassandra, Riak (always-on shopping carts).</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 6 -->
                <details class="chapter">
                    <summary>6. Replication Lag & Consistency Guarantees</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Read-Your-Writes</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Read-your-writes consistency ensures a user sees their own modifications immediately after writing, often implemented by directing subsequent reads to the same replica or using causal tracking, preventing surprises in user sessions across distributed replicas.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Requires routing user reads to the specific leader/replica handling their write.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Monotonic Reads</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Monotonic reads guarantee that once a user observes a particular version of data, future reads will not return older versions, achieved by sticky sessions or version vectors, avoiding anomalous backward jumps in time during replica switches or failures.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Achieved by pinning a user ID to a specific replica.</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 7 -->
                <details class="chapter">
                    <summary>7. Partitioning / Sharding (Chapter 6)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Key Range Partitioning</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Key range partitioning divides data into contiguous key segments assigned to shards, supporting efficient range scans and ordered access but prone to imbalances if keys are not uniformly distributed, requiring dynamic rebalancing to mitigate hotspots.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Efficient range queries; high risk of "hotspots" if keys are sequential (e.g., timestamps).</div>
                                <div class="trade-offs"><span class="label">Example:</span> Key-Range: Retrieving orders by date. Since keys are sorted, the DB reads one continuous chunk rather than hunting across every server for the data.
                                </div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Hash Partitioning</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Hash partitioning applies a hash function to keys for even distribution across shards, promoting load balance and scalability but sacrificing key order, making range queries inefficient as they require scattering across all partitions.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Uniform data distribution (load balance); destroys ordering (no range scans).</div>
                                <div class="trade-offs"><span class="label">Example:</span> Hash: Ingesting logs from millions of sensors. Hashing the ID spreads traffic perfectly across all nodes, preventing one server from getting overloaded.</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 8 -->
                <details class="chapter">
                    <summary>8. Transactions & Isolation Levels (Chapter 7)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Read Committed</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> 
                                Ensures you only see officially saved data, preventing "dirty reads." However, values can still change between queries within your transaction if someone else commits an update.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Default in many DBs; subject to "lost updates" and "non-repeatable reads."</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Snapshot Isolation (MVCC)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Snapshot isolation "freezes" the state when you start. It achieves this by storing multiple versions of every row (MVCC) instead of overwriting. Writers create new versions, while you read the old version that matches your start time.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Solves non-repeatable reads; subject to "Write Skew" (logic based on obsolete premises).</div>
                                <div class="trade-offs"><span class="label">Write-Skew:</span> Two users read the same state but update different records. Each action is valid individually, but together they break a business rule.</div>
                                <div class="trade-offs"><span class="label">Write-Skew Example:</span> Two users read the same state but update different records. Each action is valid individually, bu</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Serializable</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Serializable isolation ensures transactions execute as if sequentially, using two-phase locking or serializable snapshot isolation to detect conflicts, providing the highest integrity but at the expense of reduced concurrency and potential deadlocks.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Perfect integrity; performance penalty via 2PL (locking) or SSI (abort rates).</div>
                                <div class="trade-offs"><span class="label">Example: Applied AI: Model Registry Governance

                                    The Scenario: You have a strict governance policy: "A production AI model can only be deployed if it has passed safety checks, and only one model can be marked as 'Production' at a time to prevent version conflicts."
                                    
                                    The Race Condition (Without Serializable): Engineer A promotes Model v1.1 to "Production." At the same exact moment, Engineer B promotes Model v1.2. Both transactions check the database, see that the "Production" slot is currently empty (or holds v1.0), and both successfully write their status. You now have two models marked as "Production," breaking your governance tool.
                                    
                                    With Serializable Isolation: The database detects that both transactions read the same "current state" and tried to modify it. It forces the operations to happen serially. The first one wins; the second one fails immediately because the state it relied upon (Production slot is empty) is no longer true.
                                    
                                     </span> </div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 9 -->
                <details class="chapter">
                    <summary>9. Distributed Consistency & Consensus (Chapters 8 & 9)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Linearizability (Strong Consistency) [RECENCY GUARANTEE]</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Linearizability makes a distributed system appear as a single coherent copy with atomic operations visible immediately, satisfying strict consistency but incurring latency from synchronization. In other words, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Network delays kill performance; system halts during partitions (CAP Theorem).</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Logical Clocks (Lamport/Vector)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> 
                                Logical clocks use counters, not wall clocks, to track the sequence of events. They capture cause-and-effect relationships, allowing systems to correctly order updates and spot conflicts without needing perfectly synchronized time.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Orders events accurately without relying on unreliable physical (NTP) clocks.</div>
                                <div class="trade-offs"><span class="label">How Logical Clocks Track Event Order:</span> Compare counters: the event with the higher number is the most recent one.
                                </div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Consensus Algorithms (Paxos/Raft)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Consensus algorithms let servers agree on data despite failures. They use a majority vote to elect a leader who coordinates writes. Once more than half the nodes confirm an update, it becomes the "truth," ensuring the system stays consistent.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Essential for leader election/atomic broadcast/consistent states; slow/blocking if quorum is lost.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Two-Phase Commit (2PC)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Two-phase commit coordinates atomic transactions across participants.Nodes vote on the transaction in a 'prepare' phase. If unanimous, the coordinator executes it. If the coordinator fails, nodes remain blocked holding locks, waiting for the final decision.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Blocks if coordinator fails; poor fault tolerance compared to consensus.</div>
                            </div>
                        </details>

                    </div>
                </details>

            </div>
        </details>

        <!-- PART III -->
        <details class="part">
            <summary>Part III: Derived Data</summary>
            <div class="part-content">
                <div class="part-focus">
                    <strong>Focus:</strong> Batch Processing, Stream Processing, and Architecture Patterns.
                </div>

                <!-- Chapter 10 -->
                <details class="chapter">
                    <summary>10. Batch Processing (Chapter 10)</summary>
                    <div class="chapter-content">
                        
                        <details class="subsection">
                            <summary>Batch Output: Offline Build, Online Load</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> A highly efficient pattern where batch jobs generate pre-formatted database files (like SSTables) to be bulk-loaded, avoiding the overhead of individual network write requests.<br>
                                
                                <div class="examples">
                                    <span class="label">EXAMPLE SCENARIO:</span> <strong>Updating a "Threat Intelligence" Database</strong><br><br>
                        
                                    <!-- Step 1 -->
                                    <strong>[ STEP 1 ] The Calculation</strong><br>
                                    Your Hadoop/Spark cluster crunches the numbers (terabytes of drone footage/signals). It identifies 1 million bad guys. <br>
                                    <em>Status: Data is currently in batch processor memory.</em><br><br>
                        
                                    <!-- Step 2 -->
                                    <strong>[ STEP 2 ] The Formatting (The Magic)</strong><br>
                                    Instead of sending 1 million network requests to the DB, the Batch Job acts as a database engine:<br>
                                    <ul>
                                        <li>It asks: "What format does the DB use?" (e.g., RocksDB).</li>
                                        <li>It <strong>sorts</strong> the keys and <strong>compresses</strong> the data.</li>
                                        <li>It builds the <strong>index</strong>.</li>
                                        <li>It saves a binary file (e.g., <code>threats_2025_01.sst</code>) to the Distributed File System (S3/HDFS).</li>
                                    </ul>
                                    <em>Note: The live database is completely unaware. Zero load impact.</em><br><br>
                        
                                    <!-- Step 3 -->
                                    <strong>[ STEP 3 ] The Transfer</strong><br>
                                    The system copies the <code>.sst</code> file from batch storage to the live database server's local disk.<br><br>
                        
                                    <!-- Step 4 -->
                                    <strong>[ STEP 4 ] The Ingestion</strong><br>
                                    You send a tiny command to the DB Server: <em>"Check folder X, I put a file there."</em><br>
                                    Since the file is already sorted and indexed, the server simply "mounts" it.<br>
                                    <br>
                                    <span style="border: 1px solid var(--text-color); padding: 2px 5px;">RESULT:</span> Time: <strong>Milliseconds</strong> // CPU Load: <strong>Near Zero</strong>
                                </div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>MapReduce</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> MapReduce processes large datasets by mapping inputs to key-value pairs, shuffling and sorting by key, then reducing to aggregate outputs, with fault tolerance via task restarts and disk materialization, suited for offline computations on immutable data.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> High fault tolerance (materializes state to disk); high latency (slow for iterative jobs).</div>
                                <div class="use-cases"><span class="label">Use Cases:</span> Search indexing, nightly reports.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Dataflow Engines (Task Fusion/ Data Locality)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Dataflow engines analyze the DAG [Directed Acyclic Graph] to spot "narrow" dependencies—steps where data doesn't need to move between nodes. The engine fuses these steps into a single physical task. This preserves data locality, processing everything on one machine without expensive network shuffles or disk writes. By chaining operations in memory, the engine executes pipelines as one efficient stream rather than separate, slower steps.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> significantly faster than MapReduce; higher memory cost; recomputes lineage on failure.</div>
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 11 -->
                <details class="chapter">
                    <summary>11. Stream Processing (Chapter 11)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Log-Based Message Brokers (Similar to Event Bus)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Producers append messages to immutable partitioned logs called topics. Each message gets an offset. Consumers in groups track offsets independently, reading at their pace or replaying history. Partitions are split across several servers (brokers) parallelizing workloads, making everything faster. Replication makes copies of the data on different servers, so if one server crashes, your data stays safe and ready to use.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Replayable (time-travel); buffers load spikes; supports decoupling producers/consumers.</div>
                                <div class="trade-offs"><span class="label">Topic:</span> </div> A topic is a named stream of messages (e.g., "orders" or "page-views"). Producers append to it, and consumers subscribe to read from it. Topics are split into partitions for scalability.
                                <div class="trade-offs"><span class="label">Offset:</span> An offset is just a number that points to a specific message in the log, like a page number in a book. Consumers remember the last offset they read to pick up from there next time. </div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Change Data Capture (CDC) (Dashboard Creation)</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Change Data Capture (CDC) reads database changes from the Write-Ahead Log (WAL) or triggers, turning inserts/updates/deletes into a real-time stream of events. This decouples the source DB from multiple consumers (e.g., dashboards, analytics), each processing the same changes independently without overloading the database.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Allows derived data systems (search/cache) to sync with the primary DB reliably.</div>
                                <div class="trade-offs"><span class="label">Trigger:</span> A database trigger is a rule that automatically runs custom code when a table change (insert, update, delete) occurs, allowing CDC to capture and stream those events.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Windowing </summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Windowing chops an endless stream of data into manageable time blocks for analysis, handling late arrivals to keep results accurate.<br><br>
                                
                                <strong>4 Practical Ways to Slice Time:</strong>
                                <ul>
                                    <li>
                                        <strong>[ Tumbling Window ]</strong><br>
                                        Distinct, non-overlapping segments.<br>
                                        <em>Example: 1-minute financial candlestick charts.</em>
                                    </li>
                                    <br>
                                    <li>
                                        <strong>[ Hopping Window ]</strong><br>
                                        Fixed windows that overlap and update periodically.<br>
                                        <em>Example: Calculating a 30-second average every 5 seconds.</em>
                                    </li>
                                    <br>
                                    <li>
                                        <strong>[ Sliding Window ]</strong><br>
                                        A continuous "rolling" view that moves with the data and has no fixed boundaries.
                                    </li>
                                    <br>
                                    <li>
                                        <strong>[ Session Window ]</strong><br>
                                        Dynamic windows based on behavior. Groups a user's burst of activity and closes only after a specific period of inactivity.
                                    </li>
                                </ul>
                        
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Must handle "Event Time" (when it happened) vs "Processing Time" (when it arrived).</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Event Time vs. Processing Time [TIME-DEPENDENT JOINS]</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Determining <em>when</em> something happened is crucial for correctness.<br>
                                
                                <ul>
                                    <li><strong>Event Time:</strong> When the event occurred in the real world (e.g., user clicked at 2:00 PM). <br><em>Ensures accuracy even if network lag delivers the event at 2:05 PM.</em></li>
                                    <li><strong>Processing Time:</strong> When the server received the event. <br><em>Easier to implement, but results skew if the system lags or data is out-of-order.</em></li>
                                </ul>
                        
                                <br>
                                <span class="label">CONCEPT: Time-Dependent Joins</span><br>
                                Correlating an event to the <strong>exact state of the world at that specific moment</strong>. It aligns streams by timestamp to analyze reality as it existed <em>then</em>, rather than how it looks <em>now</em>.
                        
                                <div class="examples">
                                    <span class="label">3 STRATEGIES FOR JOINING STREAMS:</span><br><br>
                        
                                    <!-- STRATEGY 1 -->
                                    <strong>1. Stream-Stream Join (Window Join)</strong><br>
                                    A general-purpose operation for correlating two unbounded streams using time windows or watermarks to "wait" for matching events.<br>
                                    
                                    <!-- EXAMPLE 1: INFRASTRUCTURE -->
                                    <div style="margin-top: 8px; margin-bottom: 15px; border: 1px dashed var(--dim-color); padding: 10px; background: rgba(0, 20, 0, 0.5);">
                                        <span class="label">SCENARIO:</span> <strong>INFRASTRUCTURE / SMART BRIDGE</strong><br>
                                        <em>Goal:</em> Detect if a specific heavy truck caused a dangerous structural vibration.<br><br>
                                        
                                        <ul style="margin-top: 5px;">
                                            <li><strong>Stream 1 (Traffic):</strong> Weigh-In-Motion sensor logs a 40-ton truck entering "Segment A" at <span style="color: #fff;">14:00:05</span>.</li>
                                            <li><strong>Stream 2 (Structural):</strong> Strain gauge on "Segment A" reports critical vibration at <span style="color: #fff;">14:00:07</span>.</li>
                                        </ul>
                                        
                                        <strong>The Join Logic:</strong><br>
                                        Using a 5-second window, the system links the "Heavy Load" event to the "High Stress" event based on Location ID.<br>
                                        <em>Result:</em> Immediate alert confirming <strong>this specific truck</strong> caused the stress.
                                    </div>
                        
                                    <!-- STRATEGY 2 -->
                                    <strong>2. Stream-Table Join (Stream Enrichment)</strong><br>
                                    Enriches continuous real-time events by matching them against a static (or slowly changing) dataset.<br>
                                    <em>Constraint:</em> <strong>Requires State.</strong> The processor must buffer/remember events if the table update arrives slightly later than the stream event.<br>
                        
                                    <!-- EXAMPLE 2: DEFENSE -->
                                    <div style="margin-top: 8px; margin-bottom: 15px; border: 1px dashed var(--dim-color); padding: 10px; background: rgba(0, 20, 0, 0.5);">
                                        <span class="label">SCENARIO:</span> <strong>DEFENSE / AIRSPACE SURVEILLANCE</strong><br>
                                        <em>Goal:</em> Identify if a detected airborne object is hostile.<br><br>
                                        
                                        <ul style="margin-top: 5px;">
                                            <li><strong>Stream (Radar):</strong> Radar reports unidentified track entering restricted airspace at <span style="color: #fff;">15:30:00</span>, speed 800 km/h.</li>
                                            <li><strong>Table (Registry):</strong> Static/slowly-changing table of friendly aircraft IDs, transponders, and flight plans.</li>
                                        </ul>
                                        
                                        <strong>The Join Logic:</strong><br>
                                        The system enriches the radar detection by looking up the track's ID in the registry table. <strong>No match = Flagged.</strong><br>
                                        <em>Result:</em> Immediate alert for possible enemy intrusion, distinct from friendly flights.
                                    </div>
                        
                                    <!-- STRATEGY 3 -->
                                    <strong>3. Table-Table Join</strong><br>
                                    Merges two changing datasets into a single live view (Materialized View).<br>
                                    <em>Use Case:</em> Dashboards where complex links between shifting data sources must be instantly visible without constant recalculation.
                                </div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Stream Fault Tolerance</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Strategies to ensure data integrity and state recovery when stream processors fail.<br><br>
                        
                                <ul>
                                    <li>
                                        <strong>[ Rolling Checkpoints ]</strong><br>
                                        Periodically takes a 'snapshot' of the global state to durable storage.<br>
                                        <em>Function:</em> Allows the system to crash and restart from the last known good state rather than recomputing from the beginning.
                                    </li>
                                    <br>
                                    <li>
                                        <strong>[ Microbatching ]</strong><br>
                                        Breaks the continuous stream into small, discrete blocks and treats each block like a miniature batch process.<br>
                                        <em>Impact:</em> Naturally creates fixed-size windows (equal to batch size). For larger windows, state must be explicitly carried over between microbatches.
                                    </li>
                                </ul>
                            </div>
                        </details>


                        <details class="subsection">
                            <summary>Exactly-Once Processing</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Exactly-once semantics guarantee each input affects outputs precisely once despite failures, using idempotency, checkpoints, and transactional sinks, avoiding duplicates or losses, crucial for financial or analytical accuracy in fault-prone distributed streams.
                            </div>
                        </details>

                    </div>
                </details>

                <!-- Chapter 12 -->
                <details class="chapter">
                    <summary>12. Architecture Patterns (Chapter 12)</summary>
                    <div class="chapter-content">

                        <details class="subsection">
                            <summary>Lambda Architecture</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Lambda architecture combines a batch layer for comprehensive, immutable computations with a speed layer for low-latency approximations on recent data, merging results in a serving layer, but duplicating logic across paradigms increases maintenance burdens.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Operational complexity of maintaining two codebases for the same logic.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Kappa Architecture</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Kappa architecture unifies processing via streams alone, treating batch as historical stream replays on immutable logs, simplifying codebases and operations while requiring robust stream engines and sufficient storage for long-term event retention.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Conceptually simpler; requires long/infinite log retention.</div>
                            </div>
                        </details>

                        <details class="subsection">
                            <summary>Unbundled Database</summary>
                            <div class="subsection-content">
                                <span class="label">Description:</span> Unbundled databases decompose traditional DBs into specialized components like logs (Kafka), storage (S3), and indexes (Elasticsearch), allowing mix-and-match for custom systems, offering flexibility but demanding expertise in integration and monitoring.<br>
                                <div class="trade-offs"><span class="label">Trade-offs:</span> Maximum flexibility; high operational complexity (wiring/maintenance).</div>
                            </div>
                        </details>

                    </div>
                </details>

            </div>
        </details>

    </div>

    <script>
        // Optional: Smoothly scroll to the section when opened
        document.querySelectorAll('details').forEach((detail) => {
            detail.addEventListener('toggle', (e) => {
                if (detail.open) {
                    // Logic for exclusive accordion behavior could go here, 
                    // but keeping multiple sections open is often better for studying.
                }
            });
        });
    </script>
</body>
</html>